---
title: "Multiple Auxillary Variable (MAV) Models"
date: "22 July 2025"
author: "Brian Baird and Smitom Borah"
output: html_notebook
---

# Description
The following code is used to identify nitrogen or phosphours limiation. Two models are included: the MAV model after backwards variable selection (used to obtain a distribution of the critical ratio for each lake), and a bayesian model predicting the summertime mean N:P ratio (used to obtain a distribution of the observed N:P ratio for each lake). These two distributions are then compared to one another in order to assess the confidence level in which a lake is identified as nitrogen or phosphours limited. The output is exported to a CSV file.

# Packages
```{r echo=TRUE}
# List of required packages
packages <- c("nimble", "coda", "ggplot2", "tidyverse", "ggside", "psych")

# Install any that are not already installed
install_if_missing <- function(p) {
  if (!requireNamespace(p, quietly = TRUE)) {
    install.packages(p)
  }
}

invisible(lapply(packages, install_if_missing))

# Load all packages
lapply(packages, library, character.only = TRUE)
```

# Data
Let us load the data to run the model. The data has been post-processed from the data sources described in the associated research article.
```{r}
# load data
bnla_final <- read.csv("data/bnla_final.csv")

# Add observed TN:TP to dataset
bnla_final <- mutate(bnla_final, tn_tp = tn / tp)
lake_mean_n_p <- bnla_final %>%
  group_by(specific_lake_bin) %>%
  summarize(mean_lake_tn_tp = mean(tn_tp)) %>%
  ungroup()
```

# MAV model
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Model Formulation
code <- nimbleCode({
  b0.int ~ dnorm(0, sd = 10)
  b0.R ~ dnorm(20, sd = 10)
  b1.R.depth ~ dnorm(0, sd = 10)
  b0.slope ~ dnorm(0, sd = 10)
  b1.slope.temp ~ dnorm(0, sd = 10)
  sigma ~ dunif(0, 10)
  s.lake ~ dunif(0,10)

  for(i in 1:n) {
    x.ln[i] <- min(x.tp[i] , x.tn[i]/max((b1.R.depth * (log(x.depth[i]) - mu.log.depth) + b0.R), 1))  ### Using original scale for slope of temperature (slope coefficient)
    y[i] ~ dnorm((b1.slope.temp * (x.T[i] - mu.T) + b0.slope)*log(x.ln[i]) + b0.int + relake[s.k[i]], sd = sigma)
  }
  for (k in 1:p) {
    relake[k] ~ dnorm(0, sd = s.lake)
  }
})

# Dimension Assignment
n=length(bnla_final$chl); n
p=length(unique(bnla_final$specific_lake_bin)); p
mu.T = mean(bnla_final$avg_temp)
mu.log.depth = mean(log(bnla_final$INDEX_SITE_DEPTH))
mu.log.tp = mean(log(bnla_final$tp))
mu.log.tp.2 = mean(log(bnla_final$tp)^2)

# Prepare data for Nimble model
constants <- list(n = n, 
                  p = p,
                  mu.T = mu.T,
                  mu.log.depth = mu.log.depth,
                  x.tp = bnla_final$tp,
                  x.tn = bnla_final$tn,
                  x.T = bnla_final$avg_temp,
                  x.depth = bnla_final$INDEX_SITE_DEPTH,
                  s.k = bnla_final$specific_lake_bin)
yt = log(bnla_final$chl)
data <- list(y = yt)
inits <- list(b0.int = 1, b1.R.depth = 1, b0.R = 1, b1.slope.temp = 1, b0.slope = 1, sigma = 1, s.lake = 1, relake=rep(0,p))

# Build/Run Model and Obtain Samples
Nmodel <- nimbleModel(code, constants = constants, data = data, inits = inits) 
NmodelMCMC <- buildMCMC(Nmodel, monitors2 = c("relake"), enableWAIC = TRUE) 
cNmodel <- compileNimble(Nmodel)
cNmodelMCMC <- compileNimble(NmodelMCMC, project=Nmodel)
n.iter = 60000 # Desired number of iterations
samples <- runMCMC(cNmodelMCMC, niter = n.iter, WAIC = TRUE, nchains = 3, samplesAsCodaMCMC = TRUE, nburnin = 20000) 

# Save model
samples_mav <- samples
```

Here is the performance of the model:

```{r}
# Model
samples <- samples_mav

# Summary of Output
summary(samples$samples) 

# Rhat Calculations
gelman.diag(samples$samples) 

# Make sample statistics a df mean values can be called directly
samp1 <- data.frame(summary(samples$samples)$statistics); samp1


# Calculate Predictions
x.ln = pmin(constants$x.tp,  constants$x.tn/pmax((samp1["b1.R.depth","Mean"] * (log(constants$x.depth) - mu.log.depth) + samp1["b0.R", "Mean"]), 1))  #limiting nutrient (units of tp)
sum(x.ln==constants$x.tp)/length(x.ln) #fraction of time p is limiting
y.hat = (samp1["b0.int", "Mean"] + log(x.ln)*(samp1["b1.slope.temp","Mean"] * (constants$x.T - mu.T) + samp1["b0.slope","Mean"])) 

## Means of dpeth and temperature
mu.log.depth
mu.T

# Calculate R2 and rmse
y.er=yt-y.hat
R2=1-sum(y.er^2)/sum((yt-mean(yt))^2); R2 
rmse <- sqrt(mean(y.er^2)); rmse
```

Saving the bR samples
```{r Interpret bR samples}
betas <- data.frame(samples$samples$chain3) %>% 
  sample_n(3000) %>%
  select(c(b1.R.depth, b0.R))
```



# Determining lake sample standard deviation
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Model Formulation
code <- nimbleCode({
  mu ~ dnorm(0, sd = 10)
  s.lake ~ dnorm(0, sd = 10)
  sigma ~ dnorm(0, sd = 10)
  
  for (k in 1:p) {
    relake[k] ~ dnorm(0, sd = s.lake)
  }
  for (i in 1:n) {
    y[i] ~ dnorm(mu + relake[s.k[i]], sd = sigma)
  }
})

# Dimension Assignment
n=length(bnla_final$specific_lake_bin); n
p=length(unique(bnla_final$specific_lake_bin)); p


# Prepare data for Nimble model
constants <- list(n = n, 
                  p = p,
                  s.k = bnla_final$specific_lake_bin)
yt = log(bnla_final$tn_tp)
data <- list(y = yt)
inits <- list(sigma = 1, s.lake = 1, relake=rep(0,p))

# Build/Run Model and Obtain Samples
Nmodel <- nimbleModel(code, constants = constants, data = data, inits = inits) 
NmodelMCMC <- buildMCMC(Nmodel, monitors2 = c("relake"), enableWAIC = TRUE)
cNmodel <- compileNimble(Nmodel)
cNmodelMCMC <- compileNimble(NmodelMCMC, project=Nmodel)
n.iter = 60000 # Determine the number of iterations
samples <- runMCMC(cNmodelMCMC, niter = n.iter, WAIC = TRUE, nchains = 3, samplesAsCodaMCMC = TRUE, nburnin = 20000) 

# Save model
samples_Lake.sd <- samples
```

```{r}
# Model
samples <- samples_Lake.sd

# Summary of Output
summary(samples$samples) 

# Rhat Calculations
gelman.diag(samples$samples) 

# Make sample statistics a df mean values can be called directly
samp1 <- data.frame(summary(samples$samples)$statistics)
samp2 <- data.frame(summary(samples$samples2)$statistics)

# Calculate Predictions
lake_y.hat = samp1["mu","Mean"] + samp2$Mean
y.hat_df <- data.frame(lake = seq(1:p), y.hat = lake_y.hat, y.bar = log(lake_mean_n_p$mean_lake_tn_tp))


# Calculate R2
y.er = y.hat_df$y.bar - y.hat_df$y.hat
R2 = 1 - sum(y.er^2)/sum((y.hat_df$y.bar-mean(y.hat_df$y.bar))^2); R2
```

# Analyze both the variation in CR and Samples

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Model
samples <- samples_Lake.sd

# Data from three chains
obs_samples1 <- data.frame(samples$samples$chain3)
obs_samples2 <- data.frame(samples$samples2$chain3)
obs_samples <- bind_cols(obs_samples1, obs_samples2) %>% sample_n(3000)
colnames(obs_samples) <-  c("mu", "s.lake", "sigma", 1:2755)

obs_long <- obs_samples %>%
  select(-c(s.lake, sigma)) %>%
  pivot_longer(cols = 2:2756, names_to = "specific_lake_id", values_to = "lake_re") %>%
  mutate(sample_n_p_log = mu + lake_re,
         sample_n_p = exp(sample_n_p_log))
obs_long$specific_lake_id <- as.numeric(obs_long$specific_lake_id)
```

```{r Analyze both the variation in CR and Samples}
avg_lake_conditions <- bnla_final %>%
  group_by(specific_lake_bin) %>%
  summarize(INDEX_SITE_DEPTH = mean(INDEX_SITE_DEPTH),
            tp = mean(tp),
            tn = mean(tn),
            log_eutro = mean(log_eutro)) %>%
  ungroup() 

n_p_limitation <- cross_join(avg_lake_conditions, betas) %>%
  mutate(critical_ratio = b1.R.depth * (log(INDEX_SITE_DEPTH) - 1.653) + b0.R) %>%
  bind_cols(arrange(obs_long, specific_lake_id)) %>% 
  mutate(P_limited = as.numeric(critical_ratio < sample_n_p))%>%
  group_by(specific_lake_bin) %>%
  summarise(P_limitation = sum(P_limited)/length(P_limited),
            crit_ratio = mean(critical_ratio),
            est_tn_tp = mean(sample_n_p),
            index_site_depth = mean(INDEX_SITE_DEPTH),
            NEI = mean(log_eutro)) %>%
  ungroup()

hist(n_p_limitation$P_limitation, breaks = 20)
hist(n_p_limitation$crit_ratio, breaks = 20)
hist(log(n_p_limitation$est_tn_tp), breaks = 20)


dat <- n_p_limitation[2:6] %>%
  mutate("\nProbability of\nP-limitation" = P_limitation,
         "Critical ratio, mass" = crit_ratio,
         "\nSummer mean N:P\nratio, mass" = est_tn_tp,
         "NEI, unitless" = NEI,
         "Depth, ln(m)" = log(index_site_depth)) %>%
  select(c("\nProbability of\nP-limitation","Critical ratio, mass","\nSummer mean N:P\nratio, mass", "NEI, unitless","Depth, ln(m)")) 

pairs.panels(dat,
      cex.labels = 1,
      cex.axis = 1)
```


# Creating a csv file
```{r Write CSV}
# Creating GIS
mapping_gis <- left_join(bnla_final, n_p_limitation)

# Write CSV file
write.csv(mapping_gis, "outputs/n_p_limitation.csv")
```













